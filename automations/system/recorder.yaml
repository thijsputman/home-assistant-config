# The Recorder's SQLite-database is on a separate NVMe USB-drive that – although
# generally stable – occasionally disconnects. It reconnects right away and the
# filesystem is automatically remounted, but Docker doesn't take very kindly to
# this: All containers using the volume need to be restarted for the volume to
# become accessible again...
# I've tried using udev rules to cope with this, but that turned out to be
# difficult to get right (and/or properly test). The below (blunt; "if the only
# tool you have is a hammer, every problem looks like a nail"-type of solution)
# appears to work better.
# It is anyway temporary of nature: I'm planning to change the machine running
# Home Assistant at the start of 2023. The problem is specific to the
# combination of device (RPi4), USB enclosure (RTL9210), and OS (Ubuntu 20.04)
# in-use — a hardware-wise identical setup running Ubuntu 22.04 doesn't suffer
# from any of these issues (it could potentially be the RTL9210 firmware, but
# the version of smartmontools on Ubuntu 20.04 is too old to check that 🤐).

# Somewhat superfluous failure-notification; left in place for now
- alias: 📱 | 💽 Notify on Recorder-component failure
  id: 28b00ed1-a3b0-4d7f-97bc-b1faf3b446d1
  trigger:
    platform: event
    event_type: system_log_event
    event_data:
      level: ERROR
      name: homeassistant.components.recorder.core
  condition: []
  action:
    # Explicitly block any further runs of this automation while the
    # notification is up (i.e. once a notification is up, no additional
    # notifications are raised)
    - service: script.persistent_notification
      data:
        group: general
        channel: Alert
        targetDevices: my
        criticalNotification: true
        title: 💽 Recorder-component failure!
        message: >-
          The recorder-component failed with the folllowing error:
          <br><br>
          {{ trigger.event.data.message }}
  mode: single
  max_exceeded: silent
- alias: 💥 | 💽 Restart container on Recorder-component failure
  id: 50e8e275-db77-4e50-9b77-604d5ed221da
  # Triggers on three errors that clearly indicate some kind of disk/IO problem
  # with the database file. Triggering on those (and not the more abundant)
  # transactional errors that ensue after the failure to prevent triggering in
  # case of non-hardware related issues. There are three triggers defined to
  # increase reliability.
  trigger:
    - platform: event
      event_type: system_log_event
      event_data:
        level: ERROR
        name: homeassistant.components.recorder.core
    - platform: event
      event_type: system_log_event
      event_data:
        level: ERROR
        name: homeassistant.components.recorder.util
  condition:
    - condition: template
      value_template: >-
        {% if trigger.event.data.name ==
          "homeassistant.components.recorder.core" %}
          {{
            trigger.event.data.message is search(
              "^Error in database connectivity during commit: Error executing "
              ~ "query: (.*) disk I/O error") or
            trigger.event.data.message is search(
              "^Unrecoverable sqlite3 database corruption detected: (.*) "
              ~ "database disk image is malformed")
          }}
        {% elif trigger.event.data.name ==
          "homeassistant.components.recorder.util" %}
          {{
            trigger.event.data.message is search(
              "The system will rename the corrupt database file")
          }}
        {% endif %}
  action:
    # Request the Home Assistant-container to be restarted
    - service: shell_command.restart_container
    - service: script.turn_on
      target:
        entity_id: script.sms_notification
      data:
        variables:
          targetDevices: my
          message: >-
            💥 Requested Home Assistant-container restart!\n
            The restart should occur somewhere within the next 5-minutes;
            please stand-by...
    - service: script.turn_on
      target:
        entity_id: script.persistent_notification
      data:
        variables:
          group: general
          channel: Alert
          tag: container_restart
          targetDevices: my
          criticalNotification: true
          title: 💥 Requested Home Assistant-container restart!
          message: >-
            It appears the NVMe-drive disconnected — requested a restart of the
            Home Assistant-container!<br>
            The restart should occur somewhere within the next 5-minutes;
            please stand-by...
    # Block further runs of this automation (warnings enabled). The restart is
    # not instantaneous — a script outside the container checks for an active
    # request every 5-minutes
    - delay: "00:10:00"
  mode: single
  max_exceeded: warning
- alias: 📱 | 💽 Notify on successful container restart
  id: 9f3418fe-874f-4d68-9ed1-48224450f03a
  trigger:
    platform: event
    event_type: system_log_event
    event_data:
      level: WARNING
      name: homeassistant.components.recorder.util
  condition:
    - condition: template
      value_template: >-
        {{
          trigger.event.data.message is search(
            "^The system could not validate that the sqlite3 database at "
            ~ ".*\.db was shutdown cleanly") or
          trigger.event.data.message is search("^Ended unfinished session")
        }}
  action:
    # Replace the previously raised failure notification with a "success"
    # notification ("success" — as it's difficult to determine if the restart
    # didn't cause any follow on issues). Again there are multiple triggers to
    # increase reliability; the automation blocks on the notification as a
    # single notification is sufficient...
    - service: script.persistent_notification
      data:
        group: general
        channel: Alert
        tag: container_restart
        targetDevices: my
        criticalNotification: true
        title: 💽 Home Assistant-container restarted due to NVMe-drive failure!
        message: >-
          A failure of the NVMe-drive lead to a Home Assistant-container
          restart — please manually ensure all is well again...
  mode: single
  max_exceeded: silent
